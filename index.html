<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- <title>CS 566 Midterm Report</title> -->
    <link rel="stylesheet" href="style.css" />
</head>
<body>

    <div class="container">
        <h1 class="title" style="color: #0A3B5D; font-size: 34px;">Traffic Congestion Trends by Bidirectional Flow Analysis</h1>
        <p style="text-align: center; color: rgb(0, 91, 187); font-size: 20px;">
            Ayush Sharma, Nico Grafe, Zhaoyang Liu<br/>
            CS 566
        </p>

        <h2>Current Progress:</h2>
        <p>
            The three of us worked on a few different methods to extract data from traffic camera videos. 
            We each used YOLO but collected different data on the same video. Our different approaches each 
            have their own strengths and drawbacks. We plan to combine the models next and incorporate the 
            best parts of each. Then we can start working on visualizing the data and perform trend analysis on it.
        </p>
        
        <h2 style="color: rgb(36, 158, 220);">Motivation</h2>

        <h2 style="color: rgb(36, 158, 220);">Our Approach</h2>

        <h3>1️⃣ Building A Vehicle Tracking System</h3>
        <h4>Traditional Method</h4>
        <p>
            Our initial approach for developing a vehicle tracking model is to apply what we have learned in CS566, 
            including a technique called <em>background subtraction</em> to only detect changes, Gaussian blur, thresholding, 
            and boundary detection. We use certain built-in functions in <code>cv2</code> (OpenCV Python module) and 
            <code>Numpy</code> to build this vehicle detection system prototype.
        </p>

        <p>
            We first use a function called <code>cv2.VideoCapture()</code> to break the video into frames, then apply Gaussian blur to denoise 
            the current frame. We have observed a performance improvement when using Gaussian blur, especially when multiple 
            vehicles appear to overlap in the frame. Then we segment the frame into its moving foreground objects and static 
            background, to only leave with the objects that we are interested in (i.e., vehicles). <em>Background subtraction</em> 
            is a valid technique in this case because the camera is static and the only moving objects in the frame are 
            those vehicles. To do that, we use the <em>Gaussian Mixture Model</em> to classify each pixel as either foreground or 
            background. After that, we perform a series of operations to refine a mask to remove noise from the frames and 
            isolate individual objects from each other through some morphological operations like erosion and dilation. 
            Based on the foreground mask, we perform <code>cv2.findContours()</code> to find the contour of each detected vehicle. To 
            filter out errors caused by noise while finding valid contours, we assume each valid contour should have a 
            relatively large area (right now the threshold is >450). Then, for each valid contour, we use a function 
            <code>cv2.rectangle()</code> to draw a bounding box around each detected vehicle.
        </p>
        <div>
            <img src="assets/m1_res1.png" width="500" height="281" style="display: block; margin-left: auto; margin-right: auto;"/>
        </div>
        <p>
            The stack of images above show some intermediate results as well as the final output when processing one frame. The picture 
            on the upper-left corner is the original frame, the picture on the upper-right corner is the initial binary foreground 
            mask, the picture on the lower-left corner is the foreground after a series of computations and filtering, the picture 
            on the lower-right corner is the final output. As we observed, even if we try to refine our mask and apply certain 
            thresholds, we can still see some extra bounding boxes show up in the result (especially for a truck where the rearview 
            mirror was mistakenly identified as a vehicle) or can't distinguish between two vehicles that are close to each other 
            but visually distinguishable. We believe that that is a drawback of using conventional Computer Vision methods which 
            are static and may not perform well when the scene involves 3d objects (for instance, SIFT performs badly on 3d objects). 
            Thus, we try to look at some modern approaches for object detection and find out <em>YOLO</em>, which is an anchor-based deep 
            learning detection method, has gained lots of attention, to further explore and address the issues we have so far.
        </p>

        <h4>Deep Leearning Based Method</h4>
        <p>
        To address the limitations of the traditional background subtraction methodwe implemented a robust Deep Learning pipeline using 
        <strong>YOLOv8</strong> (You Only Look Once) combined with <strong>ByteTrack</strong>. unlike the static background subtraction
        which relies on pixel differences, YOLOv8 detects vehicles as distinct objects (Car, Truck, Bus, Motorbike) with high confidence, 
        even in crowded frames.

        </p>
        <div>
            <img src="assets/deep_learning_example.png" width="500" height="281" style="display: block; margin-left: auto; margin-right: auto;"/>
        </div>

        <p>
        <strong>Tracking & ID Persistence:</strong> We utilized the ByteTrack algorithm to associate detections across frames. This assigns a unique ID to each vehicle, allowing us to maintain identity even when vehicles temporarily overlap. By maintaining a history buffer of the last 8 centroids (using a <code>deque</code>), we calculate a motion vector for each vehicle. This allows us to mathematically determine direction (Left, Right, Up, Down) based on the dominant axis of displacement, rather than relying solely on lane position.
        </p>

        <p>
        <strong>Bidirectional Counting & Metrics:</strong>
        Instead of simple presence detection, we implemented a virtual counting line (configurable as vertical or horizontal). A "crossing event" is recorded only when a tracked vehicle's centroid physically transitions from one side of the line to the other. This prevents double-counting stationary vehicles. This approach enabled us to extract advanced traffic engineering metrics:
        </p>
        <ul>
            <li><strong>Flow Rate:</strong> A rolling window calculation of vehicles per minute to measure real-time throughput.</li>
            <li><strong>Traffic Density:</strong> Calculated as the number of active vehicles per million pixels, providing a measure of road saturation.</li>
            <li><strong>Dwell Time:</strong> By logging entry and exit timestamps for every unique ID, we measure exactly how long vehicles remain in the frame, which serves as a proxy for congestion levels.</li>
            <li><strong>Peak Period Detection:</strong> The system bins crossing timestamps to automatically identify time intervals where traffic flow exceeds 80% of the maximum observed volume.</li>
        </ul>

        <p>
        <strong>Automated Visualization:</strong> The pipeline includes a <code>MetricsCollector</code> class that aggregates these data points and automatically generates a dashboard of visualizations, including speed distribution histograms, lane-usage heatmaps, and temporal scatter plots of crossing events.
        </p>

        <h3>2️⃣ Perspective Transformation</h3>

        <p>
            Building on an acceptable vehicle detection model, the next step is to figure out the speed of each vehicle in 
            the video. One challenge is the displacement of one vehicle in the video calculated from a series of frames does 
            not necessarily directly map to the displacement of that vehicle in reality. As we have observed, the camera is 
            at a certain angle to the actual road surface so that the coordinate system of a frame is not necessarily a scaled 
            version of the coordinate system of the real world. To overcome this problem, we perform a <em>perspective transformation</em> 
            to map the coordinates on the frame to the coordinates in the reality.
        </p>

        <div>
            <img src="assets/pt.png" width="500" height="409.34" style="display: block; margin-left: auto; margin-right: auto;"/>
        </div>
        <p>
            The main idea is depicted in the graph shown above. We first draw a ​​polygon on the video frame to indicate the region where 
            vehicles will show up in the video and record the corresponding coordinates, as indicated by A, B, C, and D. The coordinates 
            of those four points are w.r.t. the  frame's coordinate system. For simplicity, we create a quadrilateral. Then we set up a 
            region corresponding to the roads in reality from a bird eye view so that the width and height of the region are directly 
            correlated with the actual width and height of the roads. We construct it based on the average dimensions of highway lanes, 
            as well as counting the number of dashed lines on the surface, and get points A', B', C', and D' for the four corners, 
            respectively. In order to map one coordinate system w.r.t. another coordinate system, we use <code>cv2.getPerspectiveTransform()</code> to 
            create a transformer between the two coordinate systems. Later, when we want to figure out the coordinates of a point on the 
            frame, we would insert it into <code>cv2.perspectiveTransform()</code> along with the computed transformer to get the corresponding 
            coordinates in reality. By doing so, we are able to get the (approximate) coordinates of any point in the frame in real life.
        </p>

        <p>
            However, as we can see from the screenshot, the highway consists of a horizontal section followed by an inclined section so that only 
            locating four points won't be enough to recognize the existence of the hill. The more points we use, the more robust the transformation will 
            be. Thus, we are inspired by a technique called <em>homography</em> that we learned for stitching multiple images to build a panorama, and calculate a 
            homography matrix which will then be used to perform perspective transformation. The picture below shows the points that we choose (in yellow). On both 
            sides of the road, we add one more point to try to capture the transition between the straight road and the hill. We performed manual tuning, 
            but since we don't know the true dimensions of the road nor the exact location of the reference points on the actual road, the transformation 
            won't be perfect. Our focus is to explore and find a method for analyzing traffic congestion trends, so once we know the real dimension of a road, 
            this system will be able to work as expected.
        </p>
        <div>
            <img src="assets/coor.png" width="500" height="279.276" style="display: block; margin-left: auto; margin-right: auto;"/>
        </div>
        <p>
            Once we have a proper coordinate transformation defined, we are able to estimate the speed of a vehicle at any given point in time. 
            The way we calculate the displacement of a vehicle within a time interval is shown in the figure below. We let the coordinates of a 
            vehicle be the bottom-center of the bounding box around it. For two non-contiguous frames, we compute the Euclidean distance between 
            the two coordinates, and then divide it by the time that overlaps between those two frames to arrive at the speed of the vehicle.
        </p>
        <div>
            <img src="assets/distance.png" width="600" height="309.388" style="display: block; margin-left: auto; margin-right: auto;"/>
        </div>
        <p>
            A bar chart showing the average speed of vehicles in each direction for every 20 seconds is presented here.
        </p>
        <div>
            <img src="assets/bar1.png" width="500" height="363.786" style="display: block; margin-left: auto; margin-right: auto;"/>
        </div>
        <p> 
            As we can see, for every 20 seconds in one direction, the average speed is relatively the same with some acceptable variation due to the estimation 
            of road dimensions, the shape of the roads, etc., and vehicles going in the downhill direction generally have a higher speed than 
            vehicles going in the opposite direction. Those observations align with the fact that on average, the speed limit of any British 
            highway is 112km/h (this video clip is from a British highway). One thing that we noticed is the average speed of vehicles in the 
            downhill direction has a big drop in the last 20 seconds. This is because there is a big truck which moves at a much lower speed 
            compared to others. Also, the first two 20-seconds periods have a lower average speed compared to the last 20-seconds period in the 
            uphill direction. This is because during that period, a sudden influx of vehicles traveling in that direction caused slight congestion 
            on the road. This phenomenon further proves that our system can respond to traffic conditions through real-time speed monitoring, 
            thereby helping relevant departments to quickly respond to possible circumstances on the road.
        </p>

        <p>
            Given the methods we developed and a case-based feasibility study, we apply this system to a longer video clip to further observe how well it performs.
        </p>  

        <h2 style="color: rgb(36, 158, 220);">Experiments And Results</h2>

        <p>
        To demonstrate the system's utility in crisis management, we applied our Deep Learning pipeline to footage of a
         <strong>wildfire evacuation</strong>. In this scenario, the primary goal is not just counting cars, but understanding the 
         the traffic and providing insights to first responders to help them find the fastest route. By analyzing traffic patterns during a 
         natural disaster, we can derive actionable intelligence for first responders.
        </p>

        <h3>1. Monitoring Evacuation Throughput (Flow Rate)</h3>
        <p>
        During an evacuation, knowing the road's capacity versus current demand is vital. We used the <em>Flow Rate</em> metric (vehicles per minute) to measure the pulse of the evacuation.
        </p>
        <div>
            <img src="assets/flow_rate.png" width="600" style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 10px;"/>
            <p style="text-align: center; font-size: 14px; color: #555;">Figure 4: Real-time evacuation throughput. Sustained high values indicate road saturation.</p>
        </div>
        <p>
        The graph reveals a sustained "saturation phase" where the flow rate remains near the road's theoretical maximum. <strong>For emergency response</strong>, this metric is critical: if the flow rate suddenly drops to zero while density remains high, it indicates a downstream blockage (e.g., an accident or debris) trapping evacuees. Real-time alerts based on this data would allow police to immediately redirect traffic or deploy clearance crews.
        </p>

        <h3>2. Assessing Route Viability for First Responders</h3>
        <p>
        Speed distribution is a key indicator of gridlock risk. Emergency vehicles (fire trucks, ambulances) need to know if a route is passable or if it has turned into a parking lot.
        </p>
        <div>
            <img src="assets/speed_distribution.png" width="600" style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 10px;"/>
            <p style="text-align: center; font-size: 14px; color: #555;">Figure 5: Velocity distribution during evacuation. The left-skew indicates "crawl" conditions.</p>
        </div>
        <p>
        Our analysis shows a tight clustering of speeds at the lower end of the spectrum (significantly below the speed limit). This "crawl" behavior suggests that while traffic is moving, it is highly vulnerable to total gridlock. This data allows dispatchers to make informed decisions—for example, deciding <strong>not</strong> to send heavy fire apparatus down this specific corridor because the "Time to Arrival" would be too high due to congestion.
        </p>

        <h3>3. Lane Utilization & Contra-Flow Potential</h3>
        <p>
        A common tactic in hurricane or wildfire evacuations is "Contra-Flow," where inbound lanes are reversed to allow more outbound traffic. Our spatial heatmap provides the data needed to authorize such a maneuver.
        </p>
        <div>
            <img src="assets/lane_counts.png" width="600" style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 10px;"/>
            <p style="text-align: center; font-size: 14px; color: #555;">Figure 6: Lane usage intensity. Heavy usage on one side vs. empty space on the other.</p>
        </div>
        <p>
        The heatmap highlights the disparity in lane usage. If the system detects that the outbound lanes are at 100% capacity (red/hot zones) while the inbound lanes are empty (blue/cold zones), it provides the quantitative justification for Incident Commanders to initiate contra-flow measures, effectively doubling the evacuation capacity and potentially saving lives.
        </p>

        <h3>Summary: Impact on Emergency Operations</h3>
        <p>
        This experiment confirms that our bidirectional flow analysis system can serve as a "Force Multiplier" for emergency services:
        </p>
        <ul>
            <li><strong>Bottleneck Identification:</strong> By correlating high <em>Dwell Time</em> with low <em>Flow Rate</em>, we can automatically flag sections of road where people are trapped.</li>
            <li><strong>Resource Allocation:</strong> Statistical trends help predict when the evacuation surge will taper off, allowing police to redeploy traffic control officers to other critical sectors.</li>
        </ul>

        <h2 style="color: rgb(36, 158, 220);">What We Learned</h3>

        <br/><br/><br/>
        <a href="https://drive.google.com/file/d/1uJtmUkwYj-Pp8PS10ZBuSwt9v2V1Fdsd/view?usp=sharing" target="_blank" class="button">Link To Presentation</a>
    </div>

    <!-- <script src="script.js"></script> -->
</body>
</html>
